<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sysAI.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-markdown data-separator-vertical="-VV-" data-separator="-s-">
					<script type="text/template">
<!-- .slide: id="intro_week_03"  -->
<!-- .slide: data-background="#ffffff" data-transition="zoom" -->
<p style="text-align: center;font-size: 15vmin">WEEK 03 <br> Pandas EDA and Matplotlib</p>

-VV-
# So this week is all about using Python to conduct exploratory analysis (EDA).

- We will start by defining EDA
- Then you will use pandas to generate primary and advance EDA summary tables
- You will define your feature set
- And use Graphical EDA to form some initial assumptions about the data
- Finally, as a group, you will create a notebook report and export it to pdf

-VV-

<!-- “A picture is worth more than ten thousand words” Chinese proverb -->

# Online assignment

## The online assignment is simple:

- You will be separated into four groups (see next page)
- Each group gets to examine one category from the following:
    - Group A = "Alertness"
    - Group B = "Cognition"
    - Group C = "Emotion"
    - Group D = "Personality"


-VV-
# Online assignment practicals 
- All you need to do is create a notebook report containing:
    - A short introduction explaining the measures in your category (two-four paragraphs)
    - Univariate EDA summary tables for all variables including the demographics
    - Identify problems if exist across your data
    - Univariate EDA plots for all variables
    - Focus on variables of interest
    - Multi EDA summary tables 
    - Multi EDA plots
    - Short summary (two paragraphs)

-VV-

# group assignment

| Name | Group |
| :-: | :-: |
| Pietro Nickl | A |
| Nina Coy | A |
| Pat Chormai | A |
| Bojana Grujicic | B |
| Caedyn Stinson | B |
| Karla Matic | B |
| Carolin Scholl | B |
| Antonin Fourcade | C |
| Leonardo Pettini | C |
| Oliver Contier | C |
| Moritz Doerfler | D |
| Hassan Bassam | D |
| Rebekka Tenderra | D |
| | |

-s-
<!-- .slide: id="intro_week_02"  -->
<!-- .slide: data-background="#ffffff" data-transition="zoom" -->
<p style="text-align: center;font-size: 15vmin">exploratory data analysis</p>

-VV-
# What is exploratory data analysis?

- Exploratory data analysis (EDA) is a classical and under-utilized approach that helps you quickly build a relationship
with the new data.
- The EDA method is simple; all you need to do is profile and summarise a datasets main characteristics.

-VV-
# What is exploratory data analysis?


- The way data is summarised substantially directed by the type of data; however, in almost all cases, the idea is to
identify what the properties of the data can tell us beyond narrow hypothesis testing.
- EDA was first advocated by John Tukey to encourage scientists to explore the data (without any prior assumptions), and
identify interesting patterns or relationships that could lead to new data collection and experiments.

-VV-


# Common first Steps in any EDA

1. Identification of variables and data types
1. Create quantitative summary tables that describe the dataset.
1. Use domain expertise to remove redundant information
1. Create distribution summary tables to understand the data further
1. Use Graphical univariate, bivariate and multivariate exploration to examine the data
1. Identify relationships of relevance that are worthy of exploring
1. Gain some domain expertise by writing a short description for each of these

-VV-

# Different data types (Numeric VS categorical variables)

1. Data comes in two primary flavours
1. Categorical
1. Numeric

-VV-

# Different data types - Categorical

- Categorical variables describe a characteristic of a data unit (i.e. ‘what type’ or ‘which category’).
- Categorical variables are normally represented by a non-numeric value.
- A categorical/nominal variable has two or more categories,
- There is no intrinsic ordering to the classes.
- For example, sex is a categorical variable having two types (male and female) with no inherent order to the
categories.
- In contrast, gender is a categorical variable having more than two groups depending on different criteria
- Eye colour is also a categorical variable with several sub-types (blue, brown, green, red, etc.)
- In Summary, any variable without internal ordering is categorical
-VV-


# Different data types - Ordinal

- An Ordinal variable has two or more categories with an explicit internal ordering.
- For example, educational experience (none, primary, secondary, undergrad, postgrad, PhD).
- Importantly, the spacing between the values may not be the same across the levels of the variables.
- If we assign scores 1:6 to these educational levels, we would expect more substantial differences between the first
categories than the last ones.

-VV-

# Different data types - Numeric

- Numeric/quantitative variables describe a measurable quantity in the dataset (i.e. 'how many' or 'how much').
- Numeric variables can be further divided as either discrete or continuous
- Discrete numeric variable are simply integers
- A discrete variable cannot take the value of a fraction between one value and the next closest value.
- For example the number of correct response in a set of questions
- A continuous numeric variable is defined using numbers represented with decimals (i.e. floats).
- It is typically either "every possible number" or "all the positive/negative numbers."
- For example response time is a strictly positive
- In contrast, "relative BOLD activation estimate" can be both positive and negative
-VV-


# Populations, samples and distributions

- When we examine data of any kind, it is essential to remember that we are working with a sample of data points from a
wider population.
- Most of the times, we want to understand something about the wider population. Studying the sample is a compromise.
- The problem with any sample that it is a ‘noisy’ reflection of the population.
- EDA is all about exploring the current sample properties, and it is essential to remember that this may deviate from
the broader population from which it is derived.
- To reduce the effects of this noise, it is good practice to perform EDA on only a subset of the data in hand.
-VV-

# And Now to practicals
-s-

<!-- .slide: id="intro_week_02"  -->
<!-- .slide: data-background="#ffffff" data-transition="zoom" -->
<p style="text-align: center;font-size: 15vmin">Univariate EDA tables</p>


-VV-

# univariate non-graphical simple descriptive tables

- The first step in any EDA involves the calculation of summary statistics for the data set
- Univariate methods look at one variable (data column) at a time
- Let's look at the Motor category from the hcp dataset as an example
-VV-
# We start by loading the data

- We are using glob to find all the CSV files in the data directory
- Then we read both data sets as pandas data frames


```
import numpy as np
import pandas as pd
pd.options.display.precision = 2
import os, glob
filelist = glob.glob(os.path.join("learning/course_2020/data/*/*.csv"))
df_dict = pd.read_csv(filelist[1], encoding = "ISO-8859-1")
df_hcp = pd.read_csv(filelist[0], encoding = "ISO-8859-1")
```
-VV-
# Make a mask using isin method

- We are using the hcp dictionary to create a logical mask for any row with either Motor or Subject Information
- As you can see this has True values for specific rows


```
mask = df_dict.category.isin(["Motor","Subject Information"])
mask[1:10]
```
-VV-

# Make a mask using isin method (output)


    1 True
    2 True
    3 True
    4 True
    5 True
    6 True
    7 True
    8 True
    9 True
    Name: category, dtype: bool

-VV-

# Extract the relevant keys per category

- Using the mask, we will extract all the relevant keys and place them in a list


```
keys = list(df_dict.columnHeader[mask])
keys[1:10]
```

-VV-
# Extract the relevant keys per category (output)

    ['Release',
    'Acquisition',
    'Gender',
    'Age',
    'Age',
    'Age',
    'Age_in_Yrs',
    'HasGT',
    'ZygositySR']

-VV-

# Generate an index of columns in the hcp_data

- Using our keys, we intersect once again this time with the actual columns

```
index = df_hcp.columns.isin(keys)
index[1:10]
```

    array([ True, True, True, True, False, False, False, False, False])

-VV-

# Finally we can create a subset of our df

- Using iloc we are slicing the original data frame and assigning it to a new variable



```
sub_df = df_hcp.iloc[:,list(index)]
sub_df.head()
```


-VV-

# What is the shape of the data?

- Using shape, we identify the number of events and columns


```
sub_df.shape
```




    (1206, 12)

-VV-


# Sample 50% of the data for exploration

- We will now use the built-in function to subset the data frame in half
- Start by setting a seed (for reproducibility)
- Explore different seeds to confirm that this works


```
np.random.seed(1984)
sub_sub_df = sub_df.sample(frac=0.5)
sub_sub_df
```

-VV-

# Let's extract information about our data frame

- What we want to do is create a side table
- Each row will contain summary scores capturing various information from the dataset
- Let's start by confirming that pandas is encoding the datatypes correctly

```
sub_sub_df.info()
```

-VV-

# information output

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 603 entries, 325 to 761
    Data columns (total 12 columns):
    # Column Non-Null Count Dtype
    --- ------ -------------- -----
    0 Subject 603 non-null int64
    1 Release 603 non-null object
    2 Acquisition 603 non-null object
    3 Gender 603 non-null object
    4 Age 603 non-null object
    5 Endurance_Unadj 603 non-null float64
    6 Endurance_AgeAdj 603 non-null float64
    7 GaitSpeed_Comp 603 non-null float64
    8 Dexterity_Unadj 603 non-null float64
    9 Dexterity_AgeAdj 603 non-null float64
    10 Strength_Unadj 602 non-null float64
    11 Strength_AgeAdj 602 non-null float64
    dtypes: float64(7), int64(1), object(4)
    memory usage: 61.2+ KB

-VV-

# First Simple EDA Table from our data frame

- We start by assessing the categorical variables using function describe
- We transpose the table to generate a side view because what is primarily relevant in this stage is the comparison
within a metric


```
sum_cat = sub_sub_df.describe(include = 'object').T
sum_cat

```

-VV-



# this is useful, however...

- When exploring categorical variables, we would rather have a full frequency table
- A simple tabulation of the frequency of each category is the best univariate non-graphical EDA for categorical
data.
- This is simple enough to achieve using a function and several simple steps

-VV-

# Step one dummify the data

- So this is a familiar concept across many different disciplines
- We are converting categorical variables to multidimensional binary space
- We take a vector and transform it into a matrix where each class matches a column and occurrence of a class is
binary codded


```
dummy = pd.get_dummies(sub_sub_df["Gender"])
dummy.join(sub_sub_df["Gender"]).head()
```
-VV-

# Sum the dummy to get counts

- If each event is represented as one binary value then summing each column will give us the count per class


```
Count = np.sum(dummy)
Count.head()


F 337
M 266
dtype: int64
```






-VV-


# What about Proportion?

- Proportion is the relative frequency and is defined as Count divided by total


```
Proportion = Count/np.sum(Count)
Proportion.head()

F 0.56
M 0.44
dtype: float64

```





-VV-


# And Percentage?

- The percentage is just Proportion multiplied by 100


```
Percent = Proportion*100
Percent.head()

F 55.89
M 44.11
dtype: float64
```






-VV-

# lets combine these to one summary table

- To make this useful, we would ideally have this as a pandas data frame
- Furthermore, it is easier to have this as a wide/side table rather than a long one
- Remember the objective is to compare the classes in each metric


```
df = pd.DataFrame({"Count":Count,"Proportion":Proportion,"Percent":Percent}).T
display(df)
```

-VV-

# We should also add a total column
- We can just sum these values to get the total
- Unless we have some missing values, these should sum up to 1 and 100%


```
df['Total'] = [np.sum(Count),np.sum(Proportion),np.sum(Percent)]
df
```


-VV-

# lets make this into a function

- This can now be generalised to any column


```
def freq(x):
    dummy = pd.get_dummies(x)
    Count = np.sum(dummy)
    Proportion = Count/np.sum(Count)
    Percent = Proportion*100
    df = pd.DataFrame({"Count":Count,"Proportion":Proportion,"Percent":Percent}).T
    df['Total'] = [np.sum(Count),np.sum(Proportion),np.sum(Percent)]
    df.columns.name = x.name
    return df
```

-VV-

# Now lets use our function
- We can use a loop to explore the different variables


```
for cat in sum_cat.index:
    display(freq(sub_sub_df[cat]))
```

-VV-

# what about quantitative data Characteristics?
- Univariate EDA examins the distribution of the observed sample.
- The characteristics of the quantitative distribution are called the sample statistics
- These aim to approximate some properties of the actual distribution
-VV-

# using describe on Numeric data

- Using the same function (i.e. describe) on quantitative variables will generate a different summary table



```
sum_quan = sub_sub_df.describe(include = 'float').T
display(sum_quan)


-VV-
# describe covers precision

- Count gives us some idea on the precision of the data set
- The more samples you have, the more precise your estimates are
- However, this is related to variability
- e.g. many identical samples will not be helpful

-VV-

# describe also covers centrality

- The mean and the median (i.e. 50% quartile) are indicators of location or the center of the distribution
- However, there are other measures of centrality... for example:
- Geometric =
$ \left(\prod _{i=1}^{n}x_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}} $
- Truncated or trimmed mean removing values above or below some percentage

-VV-

# The geometric mean

- It is more straightforward to understand this as a function
- The geometric mean is defined as the nth root of the product of n numbers
- However, if we try to calculate the product of many numbers we reach the limits of our computer to describe large
numbers


```
result = 1
for ix,num in enumerate(sub_sub_df.Dexterity_AgeAdj):
result = result * num
if not ix%50:
print(f'{ix}:{result}')
```

    0:91.88
    50:2.0537497919595477e+101
    100:3.130700885224843e+201
    150:6.174733748515891e+300
    200:inf
    250:inf
    300:inf
    350:inf
    400:inf
    450:inf
    500:inf
    550:inf
    600:inf

-VV-
# Dealing with inf using log

- The solution is to take the mean of the log-transformed data
- And use np.exp to convert back from logarithmic to linear scale



```
def geometric(x):
    return np.exp(np.log(x).mean())
```
-VV-
# The Truncated mean as another example

- The idea here is to measure centrality using only a middle portion of the distribution
- This is done to reduce the effects of long tails
- This involves trimming percentage of observations from both ends.
- We get the number of observation from the series shape
- Calculate the number of events to remove from both side
- Then using a sorted list, we calculated the mean on a subset of the data


```
def truncated(x, percent_trimmed):
    n = x.shape[0]
    lower_index = int(percent_trimmed*n/100)
    upper_index = n - lower_index
    return np.sort(x)[lower_index:upper_index].mean()

```
-VV-

# describe also covers spread with some help

- Measuring the spread of a distribution commonly depends on spread statistics such as variance, standard deviation,
and interquartile range.
- The standard deviation is simply the square root of the variance. This means it is in the same units as the
original data, which makes it a useful measure.
- Normally distributed data, approximately 95% of the values lie within 2 sd of the mean.
- The interquartile range is the difference between the third quartile (75%) and the first quartile (25%)

-VV-

# We can make a function just like before

```
def eda(x):
    Min,Q1,Median,Q3,Max = np.nanquantile(x,[0,0.25,0.5,0.75,1])
    obj = {"n":x.shape[0],
            "NaN":x.isna().sum(),
            "Min":Min,
            "Q1":Q1,
            "Median":Median,
            "Mean":np.mean(x),
            "Q3":Q3,
            "Max":Max,
            "IQR":Q3-Q1,
            "STD":np.std(x)}
    df = pd.DataFrame(obj, index=[x.name])
    return df
eda(sub_sub_df.Dexterity_AgeAdj)
```


-VV-

# We only need shape information

- Skewness is a measure of asymmetry.
- Kurtosis is a measure of how pointy the distribution is relative to a Gaussian shape
- Let's add these to our EDA table we will understand them after we cover graphical EDA
- Last thing we want to count the number of unique non-NA values
- This can be considered a measure of the heterogeneity of the data


-VV-

# Final EDA summary table


```
def eda(x):
    Min,Q1,Median,Q3,Max = np.nanquantile(x,[0,0.25,0.5,0.75,1])
    obj = { "n":x.shape[0],
        "NaN":x.isna().sum(),
        "Min":Min,
        "Q1":Q1,
        "Median":Median,
        "Mean":np.mean(x),
        "Q3":Q3,
        "Max":Max,
        "IQR":Q3-Q1,
        "STD":np.std(x),
        "SE":np.std(x)/np.sqrt(x.shape[0]),
        "Skewness":pd.Series.skew(x),
        "Kurtosis":pd.Series.kurtosis(x),
        "nunique":pd.Series.nunique(x)}
    df = pd.DataFrame(obj, index=[x.name])
    return df

eda(sub_sub_df.Dexterity_AgeAdj)
```




-VV-


# My decent quantitive EDA table

- So this is a simple but effective approach to examine different parts of your data and gain a feel of it


```
df = pd.DataFrame()
for cat in sum_quan.index:
    df = df.append(eda(sub_sub_df[cat]))
df

```

-VV-

# Now for some plots 
-s-
<!-- .slide: id="intro_week_02"  -->
<!-- .slide: data-background="#ffffff" data-transition="zoom" -->
<p style="text-align: center;font-size: 15vmin">EDA Plots</p>

-VV-

# Univariate categorical graphical EDA

- We are going to use Matplotlib and pandas plotting capabilities
- However, you can use any packages you want (there are at least seven excellent plotting packages I know of and if
you prefer to use them be my guest)

-VV-

# Pie charts

- We will start by examining the frequency of the categorical variables using pie charts
- This is just a visual representation of the tables we produced before


```
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
plt.style.use('ggplot')

fig, ax = plt.subplots(1, 4,figsize=(16,5))

for ix,cat in enumerate(sum_cat.index):
    ax[ix].pie(sub_sub_df[cat].value_counts()/sub_sub_df.shape[0],labels=np.unique(sub_sub_df[cat]))
    ax[ix].set_title(cat)
```
-VV-
# Pie charts output
![png](img/output_67_0.png)

-VV-
# Histograms

- The most basic graph is the histogram, which is a barplot in which each bar represents the frequency (count) or
proportion (count/total count) of cases for a range of values.

-VV-
# Histograms code 
```
fig = plt.figure(constrained_layout=True,figsize=(16,8))
gs = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)
for ix,cat in enumerate(sum_quan.index):
    i,j = np.unravel_index(ix, (3,3))
    ax = fig.add_subplot(gs[i,j])
    ax.hist(sub_sub_df[cat].dropna(),20)
    plt.ylabel("count")
    plt.xlabel(cat)

```
-VV-
# Histograms output

![png](img/output_69_0.png)

-VV-
# Boxplots

- Another handy univariate graphical technique is the boxplot
- Boxplots are very good at presenting information about the central tendency, symmetry and skew, as well as
outliers.
- However, as you can see they aren't great at detecting multimodal distributions
- Boxplots show robust measures of location and spread as well as providing information about symmetry and outliers.

-VV-
# Boxplots code
```
fig = plt.figure(constrained_layout=True,figsize=(16,8))
gs = gridspec.GridSpec(ncols=7, nrows=1, figure=fig)
for ix,cat in enumerate(sum_quan.index):
    ax = fig.add_subplot(gs[ix])
    ax.boxplot(sub_sub_df[cat].dropna(),patch_artist=True,boxprops={"facecolor":'r', "color":'r'})
    plt.ylabel(cat)

```

-VV-
# Boxplots output
![png](img/output_71_0.png)

-VV-
# Quantile-normal plots

- The quantile-normal or qqplot is both the last univariate tool we will cover and an intro to the bivariate
graphical EDA plots
- The idea is to match our sample with the theoretical normal distribution
- If the data is normal, then all data points should be aligned close to the red line
- Deviation from that line is evidence of abnormality
- We will explore these next week when we will talk about dealing with abnormal data

-VV-

# Quantile-normal code 
```
import statsmodels.api as sm

fig = plt.figure(constrained_layout=True,figsize=(12,8))
gs = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)
for ix,cat in enumerate(sum_quan.index):
    i,j = np.unravel_index(ix, (2,4))
    ax = fig.add_subplot(gs[i,j])
    sm.qqplot(sub_sub_df[cat].dropna(), line='s',ax=ax)
    plt.title(cat)



```
-VV-

# Quantile-normal output

![png](img/output_73_0.png)

-VV-
# Now let's explore Multivariate non-graphical EDA tables

- These methods can become rather complicated (with large multi-dimensional datasets)
- The gist is to examine various metrics as a function of some structure

-VV-
# Cross-tabulation

- Used to cross inspect frequency across two or more categorical variables.


```
pd.crosstab(sub_sub_df.Gender, [sub_sub_df.Age], rownames=['Gender'], colnames=['Age'])
```



-VV-

# pivot_table

- A pivot table is a way to explore summary statistics as a function of different categorical structures of interest
- It is a combination between univariate EDA tables and cross-tabulation


```
pivot1 = pd.pivot_table(sub_sub_df.dropna(),
values=['Endurance_AgeAdj',"Strength_AgeAdj"],
index='Age', columns=['Gender'],
aggfunc=['mean','std','count'])
```

-VV-

# Cross-Correlation
- Correlation is probably one of the most used metrics in the world
- We can view it for now as a measure of similarity/association between variables


```
display(sub_sub_df[['Endurance_Unadj',"Strength_Unadj","GaitSpeed_Comp","Dexterity_Unadj"]].dropna().corr())
```


-VV-

# Multivariate graphical EDA

- For this, I will use seaborn that is built over Matplotlib and simplifies complex plots 
- The violin plot is a distribution plot where some method is used to estimate from the data the distribution shape
- As you can see the bimodal distribution we saw previously is entirely explained by the gender information

-VV-

# Multivariate graphical EDA code 
```
import seaborn as sns
sns.set(style="darkgrid", palette="deep", color_codes=True)
sns.set_context("notebook", font_scale=1.5, rc={"lines.linewidth": 2.5})
fig = plt.figure(figsize=(12,8))
sns.violinplot(x="Age", y="Strength_Unadj", hue="Gender",
split=True, inner="quart",
palette={"M": "r", "F": "b"},
data=sub_sub_df)
sns.despine(left=True)

```
-VV-

# Multivariate graphical EDA output


![png](img/output_82_0.png)

-VV-

# Summary

- That's it for this week 
- Next week we will continue our dive into Data science
- We will see how we can use these EDA strategies to identify and deal with outliers 





</script>
</section>
</div>
</div>

<script src="js/reveal.js"></script>
<script src="js/config.js"></script>
<script>
window.onload=function(){function a(a,b){var c=/^(?:file):/,d=new
XMLHttpRequest,e=0;d.onreadystatechange=function(){4==d.readyState&&(e=d.status),c.test(location.href)&&d.responseText&&(e=200),4==d.readyState&&200==e&&(a.outerHTML=d.responseText)};try{d.open("GET",b,!0),d.send()}catch(f){}}var
b,c=document.getElementsByTagName("*");for(b in
c)c[b].hasAttribute&&c[b].hasAttribute("data-include")&&a(c[b],c[b].getAttribute("data-include"))};
</script>	
</body>

</html>
